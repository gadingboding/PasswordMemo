在目前的flow_test目录中有一个“全流程测试”也就是模拟用户的使用场景，而不是单独对一个功能进行单元测试。

但是目前这个全流程测试有很多问题，将来我们还会加入更多的全流程测试，这里我们需要来规定一下每个全流程测试该怎么写。

# 子项目管理
flow_test是pnpm实现的monorepo下的一个子项目，所以在维护其`package.json`的时候应该考虑全局有了什么依赖，你需要按照monorepo的管理方式来管理依赖，特别是你在引用core的时候。
在`package.json`中的script，你需要注意，每当增加了一个全流程测试就应该增加一个script目标，同时还必须有一个script用来执行全部的用例。

# 目录管理
首先的一个原则是：每个全流程测试有一个独立的文件夹，在这个文件夹里面存储的代码和数据（主要是json），还有一个文件夹用来存储公共的组件，例如数据读取工具，另外还有通用数据，例如用户的配置文件，用户的webdav账户信息之类的。

目录结构规范：
```
flow_test/
├── src/
│   ├── common/                     # 公共组件
│   └── tests/
│       ├── [测试名称]/
│       │   ├── test.ts             # 测试代码
│       │   ├── data/               # 测试数据
│       │   └── logs/               # 测试日志
└── data/                           # 通用数据
    └── common/
```


# 数据管理
这里我们可以实现一个数据管理器，从管理器获取数据，而不是直接读取数据，管理器可以覆盖掉全局的配置，这样会简单一些。

数据提供机制：
- 主要使用静态JSON文件，特殊情况下支持动态生成
- 每个测试用例通过数据管理器获取数据，而不是直接读取JSON文件
- 数据管理器支持配置覆盖，测试特定配置可以覆盖全局默认配置
- 静态数据包括：用户配置、测试数据、预期结果
- 动态生成：时间戳、随机ID、密码等敏感数据

数据命名规范：
- 用户配置文件：`user-profile.json`（包含测试用户名、基础配置信息）
- 测试数据：`test-data.json`（模板定义、测试记录数据）
- WebDAV配置：`webdav-config.json`
- 预期结果：`expected-results.json`
- 使用kebab-case命名法

数据加载流程：
测试启动 → 数据管理器初始化 → 加载静态JSON → 应用测试特定覆盖 → 生成动态数据 → 提供给测试用例

千万不要去试图比较加密后的内容，只查看明文情况是否符合与其，对于加密后的情况，我们只看数据结构对不对，例如添加了template之后，是否在数据表中添加了对应项，更新完成之后的项数对不对。因为我们的密钥算法有随机性，也不要试图去用mock模拟这种随机性，否则会相当复杂。


# 测试日志
测试的过程需要被日志记录，但是我们需要保证日志是精简的，不要将每一步的具体内容全部给记录出来，但是如果出现错误，需要能够将错误发生的位置进行记录，还有就是用类似于diff的东西将预期的数据和实际的数据输出出来，由于我们的管理器数据实际上非常简单，所以你甚至可以将数据直接dump为一个json，在报告里面引用这个dump出来的json文件，我觉得这个可以作为我们公共工具的一部分，各个测试都会用的上。
我需要你在每个用例里面都输出有意义的内容而不一大堆根本无法阅读的内容，具体来说就是不要记录每一步的情况，你可以输出每一步做了什么，然后结果是否符合预期，是否出现异常，对于出现错误的，需要记录错误原因，如果是数据不一致，则需要dump。
在控制台上，你需要输出每个用例是否成功，如果出现问题，要保证日志可以定位出问题来。


